{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "dTVNu0vEqcmp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "import torchvision as tv\n",
        "import torchvision.transforms.functional as tvf\n",
        "from torchvision import io\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "T = torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dummy data\n",
        "\n",
        "def create_data(*, seed: int = 1234, n_dim: int = 1024):\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    A = torch.randn(n_dim, n_dim)\n",
        "    B = torch.randn(n_dim, n_dim)\n",
        "    C = torch.randn(n_dim, n_dim)\n",
        "\n",
        "    alpha = 3\n",
        "    beta = 1.5\n",
        "\n",
        "    return A, B, C, alpha, beta"
      ],
      "metadata": {
        "id": "vBxSKBAWqgmG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
        "\n",
        "\n",
        "# wurlitzer\n",
        "# Capture C-level stdout/stderr pipes in Python\n",
        "# More here: https://eli.thegreenplace.net/2015/redirecting-all-kinds-of-stdout-in-python/\n",
        "\n",
        "# ninja for build\n",
        "%pip install -q wurlitzer ninja\n",
        "\n",
        "%load_ext wurlitzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUA-z0JtqjX7",
        "outputId": "30308eb7-050a-4d34-c079-ff5bf74a6cd4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The wurlitzer extension is already loaded. To reload it, use:\n",
            "  %reload_ext wurlitzer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n",
        "    if isinstance(cuda_src, str):\n",
        "        cuda_src = [cuda_src]\n",
        "    if isinstance(cpp_src, str):\n",
        "        cpp_src = [cpp_src]\n",
        "\n",
        "    return load_inline(\n",
        "        cuda_sources=cuda_src,\n",
        "        cpp_sources=cpp_src,\n",
        "        functions=funcs,\n",
        "        extra_cuda_cflags=[\"-O2\"] if opt else [],\n",
        "        verbose=verbose,\n",
        "        name=\"inline_ext\",\n",
        "    )"
      ],
      "metadata": {
        "id": "WphYdrD6qp2z"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions defined by Jeremy\n",
        "\n",
        "cuda_begin = r'''\n",
        "#include <torch/extension.h>\n",
        "#include <stdio.h>\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\");\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\");\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x);\n",
        "\n",
        "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
        "\n",
        "#define get_item(A, n, i, j) ((A)[ (i) * (n) + (j) ])\n",
        "#define cdiv(a, b) ((a + b - 1) / (b))  // Implementing ceiling division\n",
        "'''"
      ],
      "metadata": {
        "id": "ABwH4-0iqxs6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src_naive = r'''\n",
        "\n",
        "__global__ void sgemm_kernel(float* matrix_a, float* matrix_b, float* matrix_c, float alpha, float beta, int dim) {\n",
        "    int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row >= dim || col >= dim) return;\n",
        "    float tmp = 0;\n",
        "    for (int i = 0; i<dim; ++i) {\n",
        "        tmp += (matrix_a[row*dim + i] * matrix_b[dim*i + col]);\n",
        "    }\n",
        "    matrix_c[row*dim + col] = alpha * tmp + beta * matrix_c[row*dim + col];\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "torch::Tensor sgemm(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta) {\n",
        "    CHECK_INPUT(matrix_a); CHECK_INPUT(matrix_b); CHECK_INPUT(matrix_c);\n",
        "    int dim = matrix_a.size(0);\n",
        "\n",
        "    dim3 tpb(16,16);\n",
        "    dim3 blocks(cdiv(dim, tpb.x), cdiv(dim, tpb.y));\n",
        "\n",
        "    sgemm_kernel<<<blocks, tpb>>>(\n",
        "        matrix_a.data_ptr<float>(),\n",
        "        matrix_b.data_ptr<float>(),\n",
        "        matrix_c.data_ptr<float>(),\n",
        "        alpha,\n",
        "        beta,\n",
        "        dim\n",
        "        );\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return matrix_c;\n",
        "}\n",
        "'''\n",
        "\n",
        "cpp_src_naive = r'''\n",
        "    torch::Tensor sgemm(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta);\n",
        "'''\n"
      ],
      "metadata": {
        "id": "I9Rp9RSarZ9F"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src_naive_reverse = r'''\n",
        "\n",
        "__global__ void sgemm_kernel_reverse(float* matrix_a, float* matrix_b, float* matrix_c, float alpha, float beta, int dim) {\n",
        "    int col = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "    int row = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row >= dim || col >= dim) return;\n",
        "    float tmp = 0;\n",
        "    for (int i = 0; i<dim; ++i) {\n",
        "        tmp += (matrix_a[row*dim + i] * matrix_b[dim*i + col]);\n",
        "    }\n",
        "    matrix_c[row*dim + col] = alpha * tmp + beta * matrix_c[row*dim + col];\n",
        "}\n",
        "\n",
        "\n",
        "torch::Tensor sgemm_reverse(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta) {\n",
        "    CHECK_INPUT(matrix_a); CHECK_INPUT(matrix_b); CHECK_INPUT(matrix_c);\n",
        "    int dim = matrix_a.size(0);\n",
        "\n",
        "    dim3 tpb(16,16);\n",
        "    dim3 blocks(cdiv(dim, tpb.x), cdiv(dim, tpb.y));\n",
        "\n",
        "    sgemm_kernel_reverse<<<blocks, tpb>>>(\n",
        "        matrix_a.data_ptr<float>(),\n",
        "        matrix_b.data_ptr<float>(),\n",
        "        matrix_c.data_ptr<float>(),\n",
        "        alpha,\n",
        "        beta,\n",
        "        dim\n",
        "        );\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return matrix_c;\n",
        "}\n",
        "'''\n",
        "\n",
        "cpp_src_naive_reverse = r'''\n",
        "    torch::Tensor sgemm_reverse(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta);\n",
        "'''\n"
      ],
      "metadata": {
        "id": "6DbPxKdr_SlQ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src_with_shared_memory = r'''\n",
        "\n",
        "const int blockdim = 32;\n",
        "\n",
        "__global__ void sgemm_kernel_with_shared_memory(float* matrix_a, float* matrix_b, float* matrix_c, float alpha, float beta, int dim) {\n",
        "\n",
        "    __shared__ float As[blockdim][blockdim];\n",
        "    __shared__ float Bs[blockdim][blockdim];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    int row = blockIdx.y * blockdim + ty;\n",
        "    int col = blockIdx.x * blockdim + tx;\n",
        "\n",
        "    float tmp = 0.0f;\n",
        "\n",
        "    int n_tiles = cdiv(dim, blockdim);\n",
        "\n",
        "    for (int tile = 0; tile < n_tiles; ++tile) {\n",
        "        int a_col = tile * blockdim + tx;\n",
        "        int b_row = tile * blockdim + ty;\n",
        "\n",
        "        // Loading data on shared memory\n",
        "        As[ty][tx] = (row < dim && a_col < dim) ? get_item(matrix_a, dim, row, a_col) : 0.0f;\n",
        "        Bs[ty][tx] = (col < dim && b_row < dim) ? get_item(matrix_b, dim, b_row, col) : 0.0f;\n",
        "        __syncthreads();\n",
        "\n",
        "        // Perform computation using shared memory\n",
        "        for (int i = 0; i < blockdim; ++i) {\n",
        "            tmp += As[ty][i] * Bs[i][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Store result back in matrix_c\n",
        "    if (row < dim && col < dim) {\n",
        "        matrix_c[row * dim + col] = alpha * tmp + beta * matrix_c[row * dim + col];\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor sgemm_with_shared_memory(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta) {\n",
        "    CHECK_INPUT(matrix_a); CHECK_INPUT(matrix_b); CHECK_INPUT(matrix_c);\n",
        "\n",
        "    int dim = matrix_a.size(0);\n",
        "\n",
        "    dim3 tpb(blockdim, blockdim);\n",
        "    dim3 blocks(cdiv(dim, blockdim), cdiv(dim, blockdim));\n",
        "\n",
        "    sgemm_kernel_with_shared_memory<<<blocks, tpb>>>(\n",
        "        matrix_a.data_ptr<float>(),\n",
        "        matrix_b.data_ptr<float>(),\n",
        "        matrix_c.data_ptr<float>(),\n",
        "        alpha,\n",
        "        beta,\n",
        "        dim\n",
        "    );\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return matrix_c;\n",
        "}\n",
        "'''\n",
        "\n",
        "cpp_src_with_shared_memory = r'''\n",
        "    torch::Tensor sgemm_with_shared_memory(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta);\n",
        "'''\n"
      ],
      "metadata": {
        "id": "ITOACeCG-0yx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + cuda_src_naive + cuda_src_naive_reverse + cuda_src_with_shared_memory\n",
        "cpp_src = cpp_src_naive + cpp_src_naive_reverse + cpp_src_with_shared_memory\n",
        "module = load_cuda(cuda_src, cpp_src, ['sgemm', 'sgemm_reverse', 'sgemm_with_shared_memory'], verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ekwh8htrmAb",
        "outputId": "db0ae6bc-e392-460c-bbf5-b651d326a172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "The input conditions for extension module inline_ext have changed. Bumping to version 4 and re-building as inline_ext_v4...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/inline_ext/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module inline_ext_v4...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A, B, C, alpha, beta = create_data()\n",
        "a_gpu = A.contiguous().cuda()\n",
        "b_gpu = B.contiguous().cuda()\n",
        "c_gpu = C.contiguous().cuda()\n",
        "pt_out = alpha * torch.mm(A, B) + beta * C"
      ],
      "metadata": {
        "id": "mRwb1s1HuXBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -n 100 out = module.sgemm_with_shared_memory(a_gpu, b_gpu, c_gpu, alpha, beta)"
      ],
      "metadata": {
        "id": "_wFn340-Aq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A, B, C, alpha, beta = create_data()\n",
        "a_gpu = A.contiguous().cuda()\n",
        "b_gpu = B.contiguous().cuda()\n",
        "c_gpu = C.contiguous().cuda()\n",
        "pt_out = alpha * torch.mm(A, B) + beta * C"
      ],
      "metadata": {
        "id": "DpAVMn9MwNh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -n 100 out = module.sgemm(a_gpu, b_gpu, c_gpu, alpha, beta)"
      ],
      "metadata": {
        "id": "sfggBw31Aopl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A, B, C, alpha, beta = create_data()\n",
        "a_gpu = A.contiguous().cuda()\n",
        "b_gpu = B.contiguous().cuda()\n",
        "c_gpu = C.contiguous().cuda()\n",
        "pt_out = alpha * torch.mm(A, B) + beta * C"
      ],
      "metadata": {
        "id": "sSTEN7IyxDM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -n 100 out = module.sgemm_reverse(a_gpu, b_gpu, c_gpu, alpha, beta)"
      ],
      "metadata": {
        "id": "jVzX-Bad0Nz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQXOWxWl1pxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}