{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA Lesson 1: Simple matrix multiplication\n",
        "\n",
        "Author of the original: [Jeremy Howard](https://jeremy.fast.ai/), [Original notebook](https://colab.research.google.com/drive/180uk6frvMBeT4tywhhYXmz3PJaCIA_uk?usp=sharing&authuser=1), [Video](https://www.youtube.com/watch?v=4sgKnKbR-WE)\n",
        "\n",
        "\n",
        "In the first part of his lecture Jeremy writes a kernel for RGB-to-greyscale conversion. I will skip that and start from the matmul kernel.\n",
        "\n",
        "To slightly modify the algorithm I will instead implement SGEMM ($\\textbf{C} = \\alpha \\cdot \\textbf{AB} + \\beta \\textbf{C}$) with all matrices being square.\n"
      ],
      "metadata": {
        "id": "bXqfohmDHi37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Iu8FudYcIdMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "import torchvision as tv\n",
        "import torchvision.transforms.functional as tvf\n",
        "from torchvision import io\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "\n",
        "import typing as t\n",
        "\n",
        "T = torch.Tensor"
      ],
      "metadata": {
        "id": "s7_Ap1UFI1Yw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dummy data\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "n_dim = 200\n",
        "A = torch.randn(n_dim, n_dim)\n",
        "B = torch.randn(n_dim, n_dim)\n",
        "C = torch.randn(n_dim, n_dim)\n",
        "\n",
        "alpha = 3\n",
        "beta = 1.5"
      ],
      "metadata": {
        "id": "N8fKLktwJKZi"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following Jeremy I will first write the SGEMM algorithm in Python"
      ],
      "metadata": {
        "id": "Y7Nl9_MGJwZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgemm(matrix_a: T, matrix_b: T, matrix_c: T, alpha: float, beta: float):\n",
        "    out = beta * matrix_c.clone()\n",
        "    dim = matrix_a.shape[0]\n",
        "\n",
        "    for ii in range(dim):\n",
        "        for jj in range(dim):\n",
        "            for kk in range(dim):\n",
        "                out[ii, jj] += alpha * (matrix_a[ii, kk] * matrix_b[kk, jj])\n",
        "    return out"
      ],
      "metadata": {
        "id": "wVKtSIybJ3vh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace as ns\n",
        "import math\n",
        "\n",
        "def block_kernel(f, blocks, threads, *args):\n",
        "    for b0 in range(blocks.x):\n",
        "        for b1 in range(blocks.y):\n",
        "            for t0 in range(threads.x):\n",
        "                for t1 in range(threads.y):\n",
        "                    f(ns(x=b1, y=b0), ns(x=t1, y=t0), threads, *args)\n",
        "\n",
        "def sgemm_single_thread(\n",
        "    blockidx,\n",
        "    threadidx,\n",
        "    blockdim,\n",
        "    matrix_a: T,\n",
        "    matrix_b: T,\n",
        "    matrix_c: T,\n",
        "    alpha: float,\n",
        "    beta: float,\n",
        "    dim: int,\n",
        "    ):\n",
        "    col = blockidx.x * blockdim.x + threadidx.x\n",
        "    row = blockidx.y * blockdim.y + threadidx.y\n",
        "\n",
        "    if col >= dim or row >= dim:\n",
        "        return\n",
        "    tmp = 0\n",
        "    for kk in range(dim):\n",
        "        tmp += matrix_a[row*dim + kk] * matrix_b[col + kk*dim]\n",
        "    matrix_c[row*dim + col] = tmp * alpha + beta * matrix_c[row*dim + col]\n",
        "\n",
        "\n",
        "def sgemm_block(matrix_a: T, matrix_b: T, matrix_c: T, alpha: float, beta: float):\n",
        "    dim = matrix_a.shape[0]\n",
        "    tpb = ns(x=16, y=16)\n",
        "    blocks = ns(\n",
        "        x=math.ceil(dim/tpb.x),\n",
        "        y=math.ceil(dim/tpb.y),\n",
        "    )\n",
        "\n",
        "    block_kernel(\n",
        "        sgemm_single_thread,\n",
        "        blocks,\n",
        "        tpb,\n",
        "        matrix_a.flatten(),\n",
        "        matrix_b.flatten(),\n",
        "        matrix_c.flatten(),\n",
        "        alpha,\n",
        "        beta,\n",
        "        dim,\n",
        "    )\n",
        "    return matrix_c.reshape(dim, dim)"
      ],
      "metadata": {
        "id": "vQBrqbElO0dw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_out = alpha * torch.mm(A, B) + beta * C"
      ],
      "metadata": {
        "id": "vwP7l4btLwlV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sgemm_out=sgemm(A, B, C, alpha, beta)\n",
        "# assert torch.isclose(sgemm_out, pt_out, atol=1e-5).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "u8CedQo9VUk3",
        "outputId": "30c3dc29-6475-483d-9324-d661c5fb4bd7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-d76ffd719141>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msgemm_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgemm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-7efb35475888>\u001b[0m in \u001b[0;36msgemm\u001b[0;34m(matrix_a, matrix_b, matrix_c, alpha, beta)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmatrix_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmatrix_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sgemm_block_out=sgemm_block(A, B, C, alpha, beta)\n",
        "# assert torch.isclose(sgemm_block_out, pt_out, atol=1e-5).all()"
      ],
      "metadata": {
        "id": "6tZrGfc-U_xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA setup"
      ],
      "metadata": {
        "id": "IznmZ7T3NgxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> `CUDA_LAUNCH_BLOCKING=1` is a debug env variable used to block kernel launches and to report the proper stacktrace once an assert is triggered. You should not use it in production, but only during debugging.\n",
        "\n",
        "\n",
        "[source](https://discuss.pytorch.org/t/cuda-launch-blocking-1-reduces-the-training-speed/160924)"
      ],
      "metadata": {
        "id": "wL00k0S_NzIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
      ],
      "metadata": {
        "id": "HrbzK6QKMOUy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0sV0GMvANyJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wurlitzer\n",
        "# Capture C-level stdout/stderr pipes in Python\n",
        "# More here: https://eli.thegreenplace.net/2015/redirecting-all-kinds-of-stdout-in-python/\n",
        "\n",
        "# ninja for build\n",
        "%pip install -q wurlitzer ninja"
      ],
      "metadata": {
        "id": "d_uY36JpMTOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext wurlitzer"
      ],
      "metadata": {
        "id": "Pbghp1POMUnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n",
        "    return load_inline(\n",
        "        cuda_sources=[cuda_src],\n",
        "        cpp_sources=[cpp_src],\n",
        "        functions=funcs,\n",
        "        extra_cuda_cflags=[\"-O2\"] if opt else [],\n",
        "        verbose=verbose,\n",
        "        name=\"inline_ext\",\n",
        "    )"
      ],
      "metadata": {
        "id": "apIX3k0YZrqJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions defined by Jeremy\n",
        "\n",
        "cuda_begin = r'''\n",
        "#include <torch/extension.h>\n",
        "#include <stdio.h>\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\");\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\");\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x);\n",
        "\n",
        "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
        "'''"
      ],
      "metadata": {
        "id": "nohBG3TlWem2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + r'''\n",
        "__global__ void sgemm_kernel(float* matrix_a, float* matrix_b, float* matrix_c, float alpha, float beta, int dim) {\n",
        "    int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row >= dim || col >= dim) return;\n",
        "    float tmp = 0;\n",
        "    for (int i = 0; i<dim; ++i) {\n",
        "        tmp += (matrix_a[row*dim + i] * matrix_b[dim*i + col]);\n",
        "    }\n",
        "    matrix_c[row*dim + col] = alpha * tmp + beta * matrix_c[row*dim + col];\n",
        "}\n",
        "\n",
        "torch::Tensor sgemm(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta) {\n",
        "    CHECK_INPUT(matrix_a); CHECK_INPUT(matrix_b); CHECK_INPUT(matrix_c);\n",
        "    int dim = matrix_a.size(0);\n",
        "\n",
        "    dim3 tpb(16,16);\n",
        "    dim3 blocks(cdiv(dim, tpb.x), cdiv(dim, tpb.y));\n",
        "\n",
        "    sgemm_kernel<<<blocks, tpb>>>(\n",
        "        matrix_a.data_ptr<float>(),\n",
        "        matrix_b.data_ptr<float>(),\n",
        "        matrix_c.data_ptr<float>(),\n",
        "        alpha,\n",
        "        beta,\n",
        "        dim\n",
        "        );\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return matrix_c;\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "8LtN0tAPWfBu"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpp_src = \"torch::Tensor sgemm(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta);\""
      ],
      "metadata": {
        "id": "kthGn_yiZLQK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module = load_cuda(cuda_src, cpp_src, ['sgemm'], verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRTI86r2ZQx6",
        "outputId": "f49cbfa9-06d4-4ad6-f8d4-589d2d14af02"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "The input conditions for extension module inline_ext have changed. Bumping to version 1 and re-building as inline_ext_v1...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/inline_ext/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module inline_ext_v1...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py311_cu124/inline_ext/main.cpp -o main.o \n",
            "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /root/.cache/torch_extensions/py311_cu124/inline_ext/cuda.cu -o cuda.cuda.o \n",
            "[3/3] c++ main.o cuda.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o inline_ext_v1.so\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading extension module inline_ext_v1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_gpu = A.contiguous().cuda()\n",
        "b_gpu = B.contiguous().cuda()\n",
        "c_gpu = C.contiguous().cuda()"
      ],
      "metadata": {
        "id": "Uv2RN9d7ZvfS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time sgemm_cuda_output = module.sgemm(a_gpu, b_gpu, c_gpu, alpha, beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv6YWJP-h2by",
        "outputId": "f49c1a4c-7d5f-4a25-e355-e6fc402e9ded"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.66 ms, sys: 7 µs, total: 7.67 ms\n",
            "Wall time: 7.65 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff = sgemm_cuda_output.cpu() - pt_out"
      ],
      "metadata": {
        "id": "2jP1dkL8iCSR"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqHMbqC0iE--",
        "outputId": "097882bb-d4cf-4932-b444-d5a9191d79f2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0006)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cuda_src)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SietmsTnjdLe",
        "outputId": "354cd454-52d1-4cbe-ff0c-bb699e64ab48"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#include <torch/extension.h>\n",
            "#include <stdio.h>\n",
            "#include <c10/cuda/CUDAException.h>\n",
            "\n",
            "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\");\n",
            "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\");\n",
            "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x);\n",
            "\n",
            "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
            "\n",
            "__global__ void sgemm_kernel(float* matrix_a, float* matrix_b, float* matrix_c, float alpha, float beta, int dim) {\n",
            "    int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
            "    int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
            "\n",
            "    if (row >= dim || col >= dim) return;\n",
            "    float tmp = 0;\n",
            "    for (int i = 0; i<dim; ++i) {\n",
            "        tmp += (matrix_a[row*dim + i] * matrix_b[dim*i + col]);\n",
            "    }\n",
            "    matrix_c[row*dim + col] = alpha * tmp + beta * matrix_c[row*dim + col];\n",
            "}\n",
            "\n",
            "torch::Tensor sgemm(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta) {\n",
            "    CHECK_INPUT(matrix_a); CHECK_INPUT(matrix_b); CHECK_INPUT(matrix_c);\n",
            "    int dim = matrix_a.size(0);\n",
            "    \n",
            "    dim3 tpb(16,16);\n",
            "    dim3 blocks(cdiv(dim, tpb.x), cdiv(dim, tpb.y));\n",
            "\n",
            "    sgemm_kernel<<<blocks, tpb>>>(\n",
            "        matrix_a.data_ptr<float>(), \n",
            "        matrix_b.data_ptr<float>(), \n",
            "        matrix_c.data_ptr<float>(),\n",
            "        alpha,\n",
            "        beta,\n",
            "        dim\n",
            "        );\n",
            "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
            "    return matrix_c;\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cpp_src)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXqwdFjNjf3X",
        "outputId": "c5fe00a3-b550-4ccb-fd1b-8d57a608606d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch::Tensor sgemm(torch::Tensor matrix_a, torch::Tensor matrix_b, torch::Tensor matrix_c, float alpha, float beta);\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3g0OJajjiOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}